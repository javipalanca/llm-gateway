models:
  gptoss:
    container_name: vllm-gptoss
    image: vllm/vllm-openai:latest
    gpu: all
    gpus: all
    min_free_gib: 70
    priority: 100
    warm: false
    host_port: 8000
    env:
      HF_HOME: /models
      TZ: Europe/Madrid
    volumes:
    - /models:/models
    args:
    - --host
    - 0.0.0.0
    - --model
    - openai/gpt-oss-120b
    - --tensor-parallel-size
    - '2'
    - --max-model-len
    - '4096'
    - --gpu-memory-utilization
    - '0.75'
    - --enforce-eager
  deepseek-r1:
    container_name: vllm-deepseekr1
    image: vllm/vllm-openai:latest
    gpu: 1
    gpus: device=1
    min_free_gib: 55
    priority: 70
    warm: false
    host_port: 8001
    env:
      HF_HOME: /models
      TZ: Europe/Madrid
    volumes:
    - /models:/models
    args:
    - --host
    - 0.0.0.0
    - --model
    - casperhansen/deepseek-r1-distill-llama-70b-awq
    - --tensor-parallel-size
    - '1'
    - --max-model-len
    - '4096'
    - --gpu-memory-utilization
    - '0.75'
  qwen25:
    container_name: vllm-qwen25
    image: vllm/vllm-openai:latest
    gpu: 0
    gpus: device=0
    min_free_gib: 50
    priority: 60
    warm: false
    host_port: 8003
    env:
      HF_HOME: /models
      TZ: Europe/Madrid
    volumes:
    - /models:/models
    args:
    - --host
    - 0.0.0.0
    - --model
    - Qwen/Qwen2.5-72B-Instruct-AWQ
    - --tensor-parallel-size
    - '1'
    - --max-model-len
    - '4096'
    - --gpu-memory-utilization
    - '0.75'
  kimi-k2:
    container_name: vllm-kimi
    image: vllm/vllm-openai:latest
    gpu: all
    gpus: all
    min_free_gib: 65
    priority: 40
    warm: false
    host_port: 8002
    env:
      HF_HOME: /models
      TZ: Europe/Madrid
    volumes:
    - /models:/models
    args:
    - --host
    - 0.0.0.0
    - --model
    - nvidia/Kimi-K2-Thinking-NVFP4
    - --tensor-parallel-size
    - '2'
    - --max-model-len
    - '2048'
    - --gpu-memory-utilization
    - '0.70'
    - --enforce-eager
    - --trust-remote-code
  deepseek-r1-distill-qwen-32b:
    container_name: vllm-deepseek-r1-distill-qwen-32b
    image: vllm/vllm-openai:latest
    gpu: '0'
    gpus: device=0
    min_free_gib: 30
    priority: 100
    warm: false
    host_port: 8005
    env:
      HF_HOME: /models
      TZ: Europe/Madrid
    volumes:
    - /models:/models
    args:
    - --host
    - 0.0.0.0
    - --model
    - deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
    - --max-model-len
    - '2048'
    - --gpu-memory-utilization
    - '0.75'
  llama-4:
    container_name: vllm-llama-4-scout-17b-16e-mlx-text-8bit
    image: vllm/vllm-openai:latest
    gpu: all
    gpus: all
    min_free_gib: 30
    priority: 90
    warm: false
    host_port: 8006
    env:
      HF_HOME: /models
      TZ: Europe/Madrid
    volumes:
    - /models:/models
    args:
    - --host
    - 0.0.0.0
    - --model
    - lmstudio-community/Llama-4-Scout-17B-16E-MLX-text-8bit
    - --max-model-len
    - '2048'
    - --gpu-memory-utilization
    - '0.75'
  phi-2:
    container_name: vllm-phi-2
    image: vllm/vllm-openai:latest
    gpu: '0'
    gpus: device=0
    min_free_gib: 30
    priority: 60
    warm: false
    host_port: 8007
    env:
      HF_HOME: /models
      TZ: Europe/Madrid
    volumes:
    - /models:/models
    args:
    - --host
    - 0.0.0.0
    - --model
    - microsoft/phi-2
    - --max-model-len
    - '2048'
    - --gpu-memory-utilization
    - '0.75'
