models:
  gptoss:
    container_name: vllm-gptoss
    image: vllm/vllm-openai:latest
    gpu: all
    gpus: all
    min_free_gib: 70
    priority: 100
    warm: true
    host_port: 8000
    api_base: http://10.10.1.67:8000/v1
    hf_model: openai/gpt-oss-120b
    context_window_tokens: 160000
    max_output_tokens: 8192
    request_timeout: 600
    env:
      HF_HOME: /models
      TZ: Europe/Madrid
    volumes:
    - /models:/models
    args:
    - --host
    - 0.0.0.0
    - --model
    - openai/gpt-oss-120b
    - --tensor-parallel-size
    - '2'
    - --max-model-len
    - '160000'
    - --gpu-memory-utilization
    - '0.70'
    - --enforce-eager
    - --enable-prefix-caching
  deepseek-r1:
    container_name: vllm-deepseekr1
    image: vllm/vllm-openai:latest
    gpu: 1
    gpus: device=1
    min_free_gib: 55
    priority: 70
    warm: false
    host_port: 8001
    api_base: http://10.10.1.67:8001/v1
    hf_model: casperhansen/deepseek-r1-distill-llama-70b-awq
    context_window_tokens: 50000
    max_output_tokens: 4096
    force_non_stream: true
    env:
      HF_HOME: /models
      TZ: Europe/Madrid
    volumes:
    - /models:/models
    args:
    - --host
    - 0.0.0.0
    - --model
    - casperhansen/deepseek-r1-distill-llama-70b-awq
    - --tensor-parallel-size
    - '1'
    - --max-model-len
    - '50000'
    - --gpu-memory-utilization
    - '0.70'
  qwen25:
    container_name: vllm-qwen25
    image: vllm/vllm-openai:latest
    gpu: 0
    gpus: device=0
    min_free_gib: 50
    priority: 60
    warm: false
    host_port: 8003
    api_base: http://10.10.1.67:8003/v1
    hf_model: Qwen/Qwen2.5-72B-Instruct-AWQ
    context_window_tokens: 4096
    max_output_tokens: 4096
    env:
      HF_HOME: /models
      TZ: Europe/Madrid
    volumes:
    - /models:/models
    args:
    - --host
    - 0.0.0.0
    - --model
    - Qwen/Qwen2.5-72B-Instruct-AWQ
    - --tensor-parallel-size
    - '1'
    - --max-model-len
    - '4096'
    - --gpu-memory-utilization
    - '0.70'
  llama4-70b:
    container_name: vllm-llama4-70b
    image: vllm/vllm-openai:latest
    gpu: all
    gpus: all
    min_free_gib: 60
    priority: 80
    warm: false
    host_port: 8004
    api_base: http://10.10.1.67:8004/v1
    hf_model: meta-llama/Llama-3.3-70B-Instruct
    context_window_tokens: 131072
    env:
      HF_HOME: /models
      TZ: Europe/Madrid
    volumes:
    - /models:/models
    args:
    - --host
    - 0.0.0.0
    - --model
    - meta-llama/Llama-3.3-70B-Instruct
    - --tensor-parallel-size
    - '2'
    - --max-model-len
    - '131072'
    - --gpu-memory-utilization
    - '0.70'
  deepseek-r1-distill-qwen-32b:
    container_name: vllm-deepseek-r1-distill-qwen-32b
    image: vllm/vllm-openai:latest
    gpu: '0'
    gpus: device=0
    min_free_gib: 30
    priority: 100
    warm: false
    host_port: 8005
    env:
      HF_HOME: /models
      TZ: Europe/Madrid
    volumes:
    - /models:/models
    args:
    - --host
    - 0.0.0.0
    - --model
    - deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
    - --max-model-len
    - '1024'
    - --gpu-memory-utilization
    - '0.95'
  llama4-scout-17b:
    container_name: vllm-llama4-scout-17b
    image: vllm/vllm-openai:latest
    gpu: '0'
    gpus: device=0
    min_free_gib: 25
    priority: 70
    warm: false
    host_port: 8006
    api_base: http://10.10.1.67:8006/v1
    hf_model: meta-llama/Llama-4-Scout-17B-16E-Instruct
    context_window_tokens: 131072
    max_output_tokens: 4096
    env:
      HF_HOME: /models
      TZ: Europe/Madrid
    volumes:
    - /models:/models
    args:
    - --host
    - 0.0.0.0
    - --model
    - meta-llama/Llama-4-Scout-17B-16E-Instruct
    - --tensor-parallel-size
    - '1'
    - --max-model-len
    - '32768'
    - --gpu-memory-utilization
    - '0.75'
  phi-2:
    container_name: vllm-phi-2
    image: vllm/vllm-openai:latest
    gpu: '0'
    gpus: device=0
    min_free_gib: 30
    priority: 60
    warm: false
    host_port: 8007
    context_window_tokens: 2048
    max_output_tokens: 512
    env:
      HF_HOME: /models
      TZ: Europe/Madrid
    volumes:
    - /models:/models
    args:
    - --host
    - 0.0.0.0
    - --model
    - microsoft/phi-2
    - --chat-template
    - '{% for message in messages %}{% if message["role"] == "user" %}{{ message["content"] }}{% elif message["role"] == "assistant" %}{{ message["content"] }}{% endif %}{% endfor %}'
    - --max-model-len
    - '2048'
    - --gpu-memory-utilization
    - '0.75'
